# 서울대학교 윤성로 Naver Data Science Competition 2019

ailab.snu.ac.kr


Faundation
* intro
* MLB
* EM
* DevelopS
* Summary

* books
  * pattern Recognition & Machine Learninng by Bidhop
  * Deep Learning by goodfellow, bengio and courville


계산적으로 합리성을 구현하는것, rationality

rationality
rationally

expected utility 효용성

what cannot matter 자동차 연지 ㅈ

meta learning 사람만큼

the imitation game


searchenterpriseai.techtarget.com


connetionism

survellience


머신러닝의 핵심 : 일반화..  일반적인 머신의 성능.

impractical

하이 MacKay


ML의 핵심 일반적인 ML   generalization 

트래이닝때 잘 하기 못한 데이터를.. 잘 하자? 측정? 에러, 이덴. Egen

그걸 알기 위해 테스트 에러를 사용한다. 

컨시스턴스, 테스트에러 안나온다,  경기경감위해 데이터 더 받는게 학습 그게 아니면 overgittion 

failure underfitting -< high bias 추정이론, 바이어스 정의 ? 


 Capacity 수용량 능력의 크기 of a model

 the ability of the model to fit various functions

 high capa

 principle hypothese, choose  simplest,


 vc generalization VC generalization bound  svm 만든 러시아

 어퍼바운드 낮을 수록 좋다. bad 이벤트를 어퍼바운드가 vc 이밴트의 핵삼이다 

 vc 바운드 vc 디멘젼,  
 conventionsl advise


 머신러닝의 궁극의 목표 머신러닝의 trede fo ogg 어떻게 벨런링 하는지ㅣ


 approximato

 bias 



안되면 언디파팅,
바이어스를증ㅇㅇㄴ, 그거 구리===ㄱ===ㅈ


정규화,, DROPOUT WEIGHT OUT

COMPLEX MODEL + effective + big fata

# 두 시 반

ㅁㅇ



svm+커널 트릭!!

분류
    남녀노소, 
회귀
    연속적 값 예측 리그레션
바이너리ㅡ 
    확률상태 aka + logistic + regression 이런ㄱ 여러개 모아야 logistic
!


credit card application

중2 과외샘 김용대


로지스틱 

두 확률분포 거리 kl 다이버젼스, 연속과 디스크릭트에 거리 구하는 

현대적인 에러메녀 클스엔트로피, kl다이버젼스, 걔의 핵심, 0.0.1 우리의 확률, 우리의 모델 아웃픗 학률

우리의 인풋, 바이너리, 우리의 데잍는 0아니면 1 pmf 컨티뉴어스이니까 이벤트가  두 확률분포 사이의 거리 크로스엔트로피. 이걸 젤 다음에 

나의 데이터가 연속 연속저긴 것을 

나의 모델 0.76 나의 데이트는 정합, 투르폴스,

iterative iptimiaton  많은 웨잇, 백프랍을 통해 

안쪽 포룸을 파라메터,
바깥은 포룹 하이퍼라마테터 바깥 정하는게 파라메텉

둘다가 오토엠엘



